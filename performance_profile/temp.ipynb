{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem_lists = ['JOS1', 'AP1', 'AP2', 'AP4', 'FDS', 'FF1', \n",
    "# 'IKK1', 'BK1', 'DGO1', 'DGO2', 'FA1', 'FAR1', 'LOV1', 'MOP2', 'POLONI', 'SD', 'TOI4', 'TRIDIA', 'ZDT2']\n",
    "\n",
    "problem_lists = ['AP1', 'AP2', 'AP4', 'DGO2', 'FDS', 'JOS1', 'LOV1', 'MOP2', 'POLONI', \n",
    "'SD', 'TOI4', 'TRIDIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n",
      "36 36\n"
     ]
    }
   ],
   "source": [
    "admm_df = pd.read_csv('results_fact_3_admm_new/metrics.csv')\n",
    "prox_df = pd.read_csv('results_prox_new/metrics.csv')\n",
    "\n",
    "# FA1 values are NaN mostly\n",
    "prox_df = prox_df[prox_df['problem'].apply(lambda x:False if 'FA1' in x else True)]\n",
    "admm_df = admm_df[admm_df['problem'].apply(lambda x:False if 'FA1' in x else True)]\n",
    "print(len(admm_df), len(prox_df))\n",
    "\n",
    "admm_df = admm_df.sort_values(by='problem').reset_index(drop=True)\n",
    "prox_df = prox_df.sort_values(by='problem').reset_index(drop=True)\n",
    "\n",
    "def func(x):\n",
    "    if x[-2]=='l':\n",
    "        return x[:-2] + 'L1'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "prox_df['problem'] = prox_df['problem'].apply(func)\n",
    "print(len(admm_df), len(prox_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Metric**                               | **Better When** | **Explanation**                                                                         |\n",
    "| ---------------------------------------- | --------------- | --------------------------------------------------------------------------------------- |\n",
    "| **GD** (Generational Distance)           | üîΩ **Lower**    | Measures average distance to the true Pareto front. Closer = better.                    |\n",
    "| **IGD** (Inverted Generational Distance) | üîΩ **Lower**    | Measures how well the Pareto front is covered. Lower = better coverage.                 |\n",
    "| **Hypervolume**                          | üîº **Higher**   | Measures the volume dominated by the solution set. Larger = better.                     |\n",
    "| **Spacing**                              | üîΩ **Lower**    | Measures distribution uniformity. Lower = more evenly spread solutions.                 |\n",
    "| **Spread**                               | üîΩ **Lower**    | Measures extent and uniformity. Lower = better diversity and coverage.                  |\n",
    "| **Epsilon** (Additive Œµ-indicator)       | üîΩ **Lower**    | Measures how much a set must be shifted to dominate a reference. Lower = better.        |\n",
    "| **Coverage** (C-Metric)                  | üîº **Higher**   | Measures how much one set dominates another. Higher = better (if compared to baseline). |\n",
    "| **R2**                                   | üîº **Higher**   | Scalarization-based indicator. Higher = better approximation to Pareto front.           |\n",
    "| **Average Hausdorff**                    | üîΩ **Lower**    | Combines GD and IGD. Lower = closer and better coverage of Pareto front.                |\n",
    "| **Error Ratio**                          | üîΩ **Lower**    | Fraction of solutions not on the true Pareto front. Lower = better.                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_performance_profile(metric_name, df1 = admm_df, df2 = prox_df, solver_names=('ADMM', 'Proximal'), eps=1e-9):\n",
    "#     # Ensure same problem ordering and existence\n",
    "#     assert all(df1.iloc[:, 0] == df2.iloc[:, 0]), \"Problem names do not match or are not aligned.\"\n",
    "#     problems = df1.iloc[:, 0].values\n",
    "#     n_problems = len(problems)\n",
    "\n",
    "#     # Extract the metric\n",
    "#     metric1 = df1[metric_name].values + eps  # Add epsilon to avoid division by zero\n",
    "#     metric2 = df2[metric_name].values + eps  # Add epsilon to avoid division by zero\n",
    "\n",
    "#     # Stack into 2D array: rows are problems, columns are solvers\n",
    "#     results = np.vstack([metric1, metric2]).T  # shape: (n_problems, 2)\n",
    "\n",
    "#     # Compute performance ratios\n",
    "#     best = np.min(results, axis=1)\n",
    "#     ratios = results / best[:, np.newaxis]  # shape: (n_problems, 2)\n",
    "\n",
    "#     # Define œÑ grid\n",
    "#     taus = np.linspace(1, np.max(ratios)*1.05, 100)\n",
    "    \n",
    "#     # Compute œÅ_s(œÑ)\n",
    "#     rhos = []\n",
    "#     for solver_idx in range(2):\n",
    "#         rho_tau = [np.mean(ratios[:, solver_idx] <= tau) for tau in taus]\n",
    "#         rhos.append(rho_tau)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     for solver_idx, solver_name in enumerate(solver_names):\n",
    "#         plt.plot(taus, rhos[solver_idx], label=solver_name)\n",
    "#     plt.xlabel(r'$\\tau$')\n",
    "#     plt.ylabel(r'$\\rho_s(\\tau)$')\n",
    "#     plt.title(f'Performance Profile for Metric: {metric_name}')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_profile(metric, df1 = admm_df, df2 = prox_df, solver_names=('ADMM', 'Proximal'), eps=1e-5):\n",
    "    # common_problems = set(df1.iloc[:, 0]) & set(df2.iloc[:, 0])\n",
    "    # df1 = df1[df1.iloc[:, 0].isin(common_problems)].sort_values(by=df1.columns[0])\n",
    "    # df2 = df2[df2.iloc[:, 0].isin(common_problems)].sort_values(by=df2.columns[0])\n",
    "\n",
    "    # Metric column index\n",
    "    try:\n",
    "        col_idx_1 = df1.columns.get_loc(metric)\n",
    "        col_idx_2 = df2.columns.get_loc(metric)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Metric '{metric}' not found in one of the files.\")\n",
    "\n",
    "    # Extract values\n",
    "    values1 = df1.iloc[:, col_idx_1].values + eps  # Add epsilon to avoid division by zero\n",
    "    values2 = df2.iloc[:, col_idx_2].values + eps  # Add epsilon to avoid division by zero\n",
    "    results = np.vstack([values1, values2]).T  # shape: (n_problems, 2)\n",
    "\n",
    "    # Metrics where higher is better\n",
    "    higher_is_better = {\n",
    "        'Hypervolume', 'Coverage', 'R2'\n",
    "    }\n",
    "\n",
    "    if metric in higher_is_better:\n",
    "        # Flip ratio: max / t => lower is better\n",
    "        best = np.max(results, axis=1)\n",
    "        ratios = best[:, np.newaxis] / results\n",
    "    else:\n",
    "        # Normal: t / min => lower is better\n",
    "        best = np.min(results, axis=1)\n",
    "        ratios = results / best[:, np.newaxis]\n",
    "\n",
    "    # Adaptive tau range\n",
    "    tau_max = np.max(ratios) * 1.05\n",
    "    taus = np.linspace(1, tau_max, 5000)\n",
    "\n",
    "    # Compute performance profile\n",
    "    rhos = []\n",
    "    for solver_idx in range(len(solver_names)):\n",
    "        rho_tau = [np.mean(ratios[:, solver_idx] <= tau) for tau in taus]\n",
    "        rhos.append(rho_tau)\n",
    "\n",
    "    return taus, rhos, solver_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [f for f in admm_df.columns if f not in {'problem', 'IGD', 'Epsilon', 'Coverage', 'R2'}]\n",
    "\n",
    "# fig, axs = plt.subplots(4, 2, figsize=(12, 16))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for j, i in enumerate(metric_names[4:]):\n",
    "    taus, rhos, solver_names = plot_performance_profile(i, admm_df, prox_df, solver_names=('ADMM', 'Proximal'))\n",
    "    for idx in range(2):\n",
    "        axs[j//2, j%2].plot(taus, rhos[idx], label=solver_names[idx], linewidth=2)\n",
    "    \n",
    "    axs[j//2, j%2].set_xlabel(r'$\\tau$', fontsize=14)\n",
    "    axs[j//2, j%2].set_ylabel(r'Performance profile $\\rho_s(\\tau)$', fontsize=14)\n",
    "    if i=='avg_time':\n",
    "        i = 'Average Time'\n",
    "    elif i=='avg_iter':\n",
    "        i = 'Average Iterations'\n",
    "    elif i=='Average_Hausdorff':\n",
    "        i = 'Average Hausdorff'\n",
    "    elif i=='Error_Ratio':\n",
    "        i = 'Error Ratio'\n",
    "    axs[j//2, j%2].set_title(f'{i}', fontsize=15)\n",
    "    axs[j//2, j%2].legend()\n",
    "    axs[j//2, j%2].grid(False)\n",
    "\n",
    "\n",
    "    # compute_performance_profile(i, admm_df, prox_df, solver_names=('ADMM', 'Proximal'), eps=1e-9)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('performance_profiles.png', bbox_inches='tight')\n",
    "plt.savefig('performance_profile_ppt2.png', bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    }
   ],
   "source": [
    "# stichting the images together for a better comparison\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "prox_imgs = [os.path.join('results_prox_new', f) for f in os.listdir('results_prox_new') if f.endswith('.png')]\n",
    "prox_imgs = sorted(prox_imgs)\n",
    "admm_imgs = [os.path.join('results_fact_3_admm_new', f) for f in os.listdir('results_fact_3_admm_new') if f.endswith('.png')]\n",
    "admm_imgs = sorted(admm_imgs)\n",
    "\n",
    "\n",
    "# probs_to_remove = ['IKK', 'FA', 'FF1', 'DGO1']\n",
    "# for prob in probs_to_remove:\n",
    "#     prox_imgs = [f for f in prox_imgs if prob not in f]\n",
    "#     admm_imgs = [f for f in admm_imgs if prob not in f]\n",
    "\n",
    "\n",
    "new_admm_imgs, new_prox_imgs = [], []\n",
    "for prob in problem_lists:\n",
    "    temp_a = [f for f in admm_imgs if prob in f]\n",
    "    temp_b = [f for f in prox_imgs if prob in f]\n",
    "    \n",
    "    if len(temp_a) > 0 and len(temp_b) > 0:\n",
    "        new_admm_imgs.append(temp_a)\n",
    "        new_prox_imgs.append(temp_b)\n",
    "#     admm_imgs = [f for f in admm_imgs if prob in f]\n",
    "#     prox_imgs = [f for f in prox_imgs if prob in f]\n",
    "\n",
    "# len(prox_imgs), len(admm_imgs)\n",
    "\n",
    "# admm_imgs = np.concatenate(new_admm_imgs).tolist()\n",
    "# prox_imgs = np.concatenate(new_prox_imgs).tolist()\n",
    "# admm_imgs = sorted(admm_imgs)\n",
    "# prox_imgs = sorted(prox_imgs)\n",
    "admm_imgs = new_admm_imgs\n",
    "prox_imgs = new_prox_imgs\n",
    "print(len(admm_imgs), len(prox_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 3 images in one figure for comparison\n",
    "# for prox_path, admm_path in zip(prox_imgs, admm_imgs):\n",
    "num_imgs_single = 2\n",
    "for i in range(0, len(admm_imgs), num_imgs_single):\n",
    "    # print(f\"i = {i}\")\n",
    "    # Create a new figure with two subplots\n",
    "    fig, axs = plt.subplots(num_imgs_single, 2, figsize=(12, num_imgs_single * 4))\n",
    "    for j in range(i, min(i + num_imgs_single, len(prox_imgs))):\n",
    "        # print(f\"j = {j}\")\n",
    "        # prox_img = plt.imread(prox_imgs[j])\n",
    "        admm_img1 = plt.imread(admm_imgs[j][0])\n",
    "        admm_img2 = plt.imread(admm_imgs[j][1])\n",
    "\n",
    "        axs[j - i, 0].imshow(admm_img1)\n",
    "        axs[j - i, 0].axis('off')\n",
    "        # axs[j - i, 0].set_title(f'Proximal')\n",
    "\n",
    "        axs[j - i, 1].imshow(admm_img2)\n",
    "        axs[j - i, 1].axis('off')\n",
    "            # axs[j - i, 1].set_title(f'ADMM')\n",
    "\n",
    "\n",
    "        # axs[0].imshow(prox_img)\n",
    "        # axs[0].axis('off')\n",
    "        # axs[0].set_title('Proximal')\n",
    "\n",
    "        # axs[1].imshow(admm_img)\n",
    "        # axs[1].axis('off')\n",
    "        # axs[1].set_title('ADMM')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'admm_results_ppt_{i//num_imgs_single}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admm_df = admm_df[~admm_df['problem'].str.contains('|'.join(probs_to_remove))].reset_index(drop=True)\n",
    "# prox_df = prox_df[~prox_df['problem'].str.contains('|'.join(probs_to_remove))].reset_index(drop=True)\n",
    "admm_df = admm_df[admm_df['problem'].str.contains('|'.join(problem_lists))].reset_index(drop=True)\n",
    "prox_df = prox_df[prox_df['problem'].str.contains('|'.join(problem_lists))].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# cols_to_select = ['Problem', 'g(x)'] +  list(admm_df.columns[1:])\n",
    "cols_to_select = ['Problem', 'g(x)'] + metric_names + ['R2', 'Coverage']\n",
    "\n",
    "admm_df['Problem'] = admm_df['problem'].apply(lambda x:x.split(',')[0])\n",
    "prox_df['Problem'] = prox_df['problem'].apply(lambda x:x.split(',')[0])\n",
    "\n",
    "admm_df['g(x)'] = admm_df['problem'].apply(lambda x:'l1' if x[-2:]=='L1' else '0')\n",
    "prox_df['g(x)'] = prox_df['problem'].apply(lambda x:'l1' if x[-2:]=='L1' else '0')\n",
    "\n",
    "\n",
    "# admm_df.loc[:, cols_to_select].to_csv('final_admm_results_table.csv', index=False)\n",
    "# prox_df.loc[:, cols_to_select].to_csv('final_prox_results_table.csv', index=False)\n",
    "problems_info = pd.read_csv('/home/vm270/ayush/expts/Multi-Objective-Optimization/methods/Final_ADMM/results_new/problems_info.csv')\n",
    "admm_df.merge(problems_info, on='Problem', how='left').loc[:, cols_to_select[:1] + ['m', 'n'] + cols_to_select[1:]].to_csv('final_admm_results_table.csv', index=False)\n",
    "prox_df.merge(problems_info, on='Problem', how='left').loc[:, cols_to_select[:1] + ['m', 'n'] + cols_to_select[1:]].to_csv('final_prox_results_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGM_ENV",
   "language": "python",
   "name": "pgm_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
